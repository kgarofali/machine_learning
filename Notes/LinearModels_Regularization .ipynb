{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Linear Models and Regularization\n",
    "\n",
    "Estimate for function we are looking for, with $x_{j}$ being the independent variables, or features. \n",
    "$ y = \\beta_{o} + \\sum_{j=1}^{p} x_{j} \\beta_{j} = \\vec X^{T} \\vec \\beta$, with $\\vec X$ being $[1,x_{i},...,x_{p}]$ as a row vector and $\\vec \\beta$ being $[\\beta_{o}, ..., \\beta{p}]$ as column vector: $\\vec \\beta = (\\vec X^{T} \\vec X)^{-1} \\vec X^{T} \\vec y$. In practice, this can be hard because inverting matrices takes time. So in reality, we would calculate: $\\vec X^{T} \\vec X \\vec \\beta = \\vec X^{T} \\vec y$ to search for value of $\\vec \\beta$. In python this is something like np.linalg.solve(). \n",
    "\n",
    "\n",
    "We can see how good our estimate is with things like the following: \n",
    "* $ RSS = \\sum_{i=1}^{N} (y_{i} - \\vec x_{i}^{T} \\vec \\beta)^{2}$ (residual sum of squares) \n",
    "\n",
    "\n",
    "## Polynomial Regression \n",
    "\n",
    "This is example of something that is really just multiple linear regressions: \n",
    "$y = \\beta_{o} + \\beta_{1} x + \\beta_{2} x^{2} + ... $ \n",
    "\n",
    "So now RSS becomes: $ RSS = \\sum_{i=1}^{N} (y_{i} - \\beta_{0} - \\beta_{1}x_{i} - \\beta_{2}x_{i}^{2})^{2}$. You may get perfect fit when you have as many terms as values (Nth order fit for N data points), but essentially you are just fitting the noise, which is ** overfitting.** You are fitting your training set very well, but when test set comes out you will not be able to fit it well. How do we decide the proper number of parameters to have? The method to use is called ** cross-validation.**\n",
    "\n",
    "## Cross-Validation \n",
    "\n",
    "Bias versus variance trade-off: **bias** is the errors in your fitting due to too few parameters (i.e. underfitting). **Variance** refers to the amount by which $\\hat f$ (approx to function) will change if we estimated it using a different training set. **Cross-validation** tells you how many free parameters to keep. \n",
    "\n",
    "### k-Fold Cross-Validation \n",
    "\n",
    "(using k=5 or k=10): assume you have some $x_{i}, y_{i}$ pairs of data. Randomly divide the data into 10 parts (for k=10). Let test set be your first data division, and the remaining division be your \"new training set\" (test set being data model hasn't seen yet). Do this and calculate RSS 10 times (increasing number of parameters from square, cubic, quadric, etc), and then plot the RSS versus the number of parameters. Optimum number of parameters is where RSS first drops to lowest value--more params doesn't get you big enough improvement. You can use this to choose k-nearest neighbors classification, bandwidth (h) in kernel density estimation, order of polynomial, etc. \n",
    "\n",
    "\n",
    "## Subset Selection \n",
    "\n",
    "What are the important features (independent variables)? \n",
    "* best subset selection: what are all subsets that rely on only one feature? or on only two features? again calculate RSS for these selections and see where it is lowest--want to see how many features your y really depends on.  \n",
    "* forward step wise selection: choose feature that gives you least RSS, then add next feature that gives you least RSS, and so on. \n",
    "* backward step wise selection: start with all features and remove one at a time (removing feature that gives least benefit). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
