{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Linear Models for Regression \n",
    "\n",
    "ML falls under unsupervised (modeling giving training set data and test set data) or supervised (classification). Linear regression is unsupervised learning given training set data that is continuous. If given discrete data, this falls under classification. \n",
    "\n",
    "Given data (training set) $y_{i}$ (response variable), ${\\vec x_{i}}$ (which is a column vector of data $x_{i1}$...$x_{ip}$, where p are features or attributes of the independent variable: $y_{i} = f({\\vec x_{i}})$ and we want to find an approximation for this f (function). We want to predict y for test set data using calculated f: $y_{j} = f({\\vec x_{j}})$. We want to do this because in general $y_{j}$ is something that is expensive to measure otherwise. \n",
    "\n",
    "The linear model, which is the simplest assumption, says $y = \\beta_{o} + \\sum_{j=1}^{p}x_{j} \\times \\beta_{j}$ , where $y = {\\vec x^{T}} {\\vec\\beta}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Example \n",
    "\n",
    "Take $y = \\beta_{o} + \\beta_{1} x + \\epsilon $ , where y is dependent var, x is scalar independent var, $\\eta$ is noise or error term, $\\beta_{o}$ is intercept, and $\\beta_{1}$ is the slope. Here we assume that noise/error does not depend on x (homoscedastic). If this is not the case, then we are in the heteroscedastic case. \n",
    "\n",
    "Given $(x_{1}, y_{1}), (x_{2}, y_{2}), etc$, we will calculate the resdiual as follows: $\\epsilon_{i} = y_{i} - \\hat y_{i}$, where $\\hat y_{i} = \\beta_{o} + \\beta_{1} x_{1}$ is basically how good the fit is. \n",
    "\n",
    "We want to test for \"goodness of fit\" based on the **residual sum of squares (rss)**: $\\phi = rss = \\sum_{i=1}^{N} (y_{i} - \\beta_{o} - \\beta_{1} x_{i})^{2}$. \n",
    "\n",
    "The ** least squares ** approach chooses $\\beta_{o}$ and $\\beta_{1}$ to minimize rss. \n",
    "\n",
    "### Minimization \n",
    "\n",
    "$\\frac{\\partial \\phi}{\\partial \\beta_{o}} = \\sum_{i=1}^{N} 2(y_{i} - \\beta_{o} -\\beta{1}x_{i})(-1) = 0 $\n",
    "\n",
    "$\\frac{\\partial \\phi}{\\partial \\beta_{1}}= \\sum_{i=1}^{N} 2(y_{i} - \\beta_{o} -\\beta{1}x_{i})(-x_{i}) = 0 $\n",
    "\n",
    "Solving for $\\beta_{o}$ and $\\beta_{1}$, the equations become: \n",
    "\n",
    "(1) $\\sum_{i=1}^{N} y_{i} - \\beta_{o} -\\beta_{1}x_{i} = 0 $\n",
    "\n",
    "(2) $\\sum_{i=1}^{N} y_{i} - \\beta_{o} -\\beta_{1}x_{i} = 0 $ \n",
    "\n",
    "Breaking (1) into component parts: \n",
    "\n",
    "$\\sum_{i=1}^{N} y_{i} - N \\beta_{o} - \\beta_{1} \\sum_{i=1}^{N} x_{i} = 0 $ \n",
    "\n",
    "$ N \\bar y - N \\beta_{o} - \\beta_{1} N \\bar x = 0 $ \n",
    "\n",
    "Breaking (2) into component parts: \n",
    "\n",
    "$\\sum_{i=1}^{N} y_{i} x_{i} - \\beta_{o} \\sum_{i=1}^{N} x_{i} - \\beta_{1} \\sum_{i=1}^{N} x_{i}^{2} = 0 $ \n",
    "\n",
    "$N \\langle x_{i} y_{i} \\rangle - \\beta_{o} N \\bar x - \\beta_{1} N \\langle x_{i}^{2} \\rangle = 0$ \n",
    "\n",
    "With two equations and two unknowns we can solve for $\\beta_{o}$ and $\\beta_{1}$: \n",
    "\n",
    "$\\beta_{1} = \\frac{\\langle x_{i} y_{i} \\rangle - \\bar x \\bar y}{\\langle x_{i}^{2} \\rangle - \\bar x^{2}}$ \n",
    "\n",
    "$ \\beta_{o} = \\bar y - \\beta_{1} \\bar x $\n",
    "\n",
    "### Evaluating Estimate\n",
    "\n",
    "Now we would like to ask: how good is our estimate of $\\beta_{o}$ and $\\beta_{1}$? You could use chisq method, bootstrap, or rsquare statistic. \n",
    "\n",
    "$R^{2}$ stat: $TSS = \\sum_{i=1}^{N} (y_{i} - \\bar y)^{2}$, where TSS is total sum of squares, and $\\bar y$ is the zeroth order model. \n",
    "\n",
    "When you do this you get $R^{2} = \\frac{TSS - RSS}{TSS}$, where we want RSS to be much smaller than TSS to $R^{2} \\approx 1$. \n",
    "* $R^{2}$ close to 1 is good fit. \n",
    "* $R^{2}$ close to zero means linear model is not  correct or errors are too large, or both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
