{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: K-nearest Neighbors and Classification \n",
    "\n",
    "\n",
    "## Expectation-maximization: \n",
    "\n",
    "$ g(y) = (1 - \\pi)\\phi_{\\theta1}(y) + \\pi \\phi_{\\theta2}(y) $ \n",
    "\n",
    "Likelihood: \n",
    "\n",
    "$L(\\theta) = \\Pi_{i~st~\\Delta_{i}~=~0} (1-\\pi)\\phi_{\\theta1}(y) \\Pi_{i~st~\\Delta_{i}~=~1} \\pi \\phi_{\\theta2}(y)$\n",
    "\n",
    "Take log of likelihood: \n",
    "\n",
    "$l(\\theta) = log(L(\\theta)) = \\sum_{i~st~\\Delta_{i}=0} [log(1-\\pi) + log\\phi_{\\theta1}(y)] + \\sum_{i~st~\\Delta_{i}=1}[log\\pi + log\\phi_{\\theta2}(y)]$ \n",
    "\n",
    "Simplifying: \n",
    "\n",
    "$= \\sum_{i=1}^{N} (1-\\Delta_{i})log(1-\\pi) + (1-\\Delta_{i})log\\phi_{\\theta1}(y)) + \\sum_{i=1}^{N} \\Delta_{i}log\\pi + \\Delta_{i}log\\phi_{\\theta2}(y)$\n",
    "\n",
    "$l(\\theta) = \\sum_{i=1}^{N} [(1-\\Delta_{i})log\\phi_{\\theta1}(y) + \\Delta_{i}log\\phi_{\\theta2}(y)] + \\sum_{i=1}^{N} [(1-\\Delta_{i})log(1-\\pi) + \\Delta_{i}log\\pi] $ \n",
    "\n",
    "## Expectation step: \n",
    "\n",
    "$\\hat \\gamma_{i} = \\frac{\\hat \\pi \\phi_{\\hat \\theta2}(y_{i})}{(1-\\hat \\pi)\\phi_{\\hat \\theta1}(y_{i}) + \\hat \\pi \\phi_{\\hat \\theta2}(y_{i})}$ \n",
    "\n",
    "where $\\hat \\gamma$ is the responsibility. We want to replace $\\Delta_{i}$ with $\\hat \\gamma_{i}$, which is a probabilistic estimate. If we put these in the $l(\\theta)$  then it's a maximization problem, which we know how to solve. \n",
    "\n",
    "## Maximization step: \n",
    "\n",
    "The following look like means and variances with some weighting factors: \n",
    "\n",
    "$\\hat \\mu_{1} = \\frac{\\sum_{i=1}^{N} (1-\\hat\\gamma_{i})y_{i}}{\\sum_{i=1}^{N} (1-\\hat \\gamma_{i})} $ \n",
    "\n",
    "$\\hat \\sigma_{1}^{2} = \\frac{\\sum_{i=1}^{N} (1-\\hat\\gamma_{i})(y_{i} - \\hat \\mu_{1})^{2}}{\\sum_{i=1}^{N}...} $\n",
    "\n",
    "$\\hat \\mu_{2} = \\frac{\\sum_{i=1}^{N} \\hat \\gamma_{i} y_{i}}{\\sum_{i=1}^{N} \\hat \\gamma_{i}} $ \n",
    "\n",
    "$\\hat \\sigma_{2}^{2} = \\frac{\\sum_{i=1}^{N} \\hat \\gamma_{i} (y_{i} - \\hat \\mu_{1})^{2}}{\\sum_{i=1}^{N} \\hat \\gamma_{i}}$\n",
    "\n",
    "$\\hat \\pi = \\frac{\\sum_{i=1}^{N} \\hat \\gamma_{i}}{N} $ \n",
    "\n",
    "\n",
    "$\\phi_{\\theta1} = \\frac{1}{\\sqrt{2\\pi}\\sigma_{1}}e^-{\\frac{(y-\\mu_{1})^{2}}{2\\sigma_{1}^{2}}}$ \n",
    "\n",
    "$\\phi_{\\theta2} = \\frac{1}{\\sqrt{2\\pi}\\sigma_{2}}e^-{\\frac{(y-\\mu_{2})^{2}}{2\\sigma_{2}^{2}}}$ \n",
    "\n",
    "$log\\phi_{\\theta1} = \\frac{-(y_{i} - \\mu_{1})^{2}}{2\\sigma_{1}^{2}} - log\\sqrt{2\\pi} - log\\sigma_{1}$ \n",
    "\n",
    "$log\\phi_{\\theta2} = \\frac{-(y_{i} - \\mu_{2})^{2}}{2\\sigma_{2}^{2}} - log\\sqrt{2\\pi} - log\\sigma_{2}$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial \\mu_{1}} = 0$ --> $\\mu_{1} = \\frac{\\sum_{i=1}^{N} (1-\\gamma_{i})y_{i}}{\\sum_{i=1}^{N}(1-\\gamma_{i})}$ \n",
    "\n",
    "same for $\\frac{\\partial l}{\\partial \\mu_{1}} = 0$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial \\pi}  = 0$ --> $\\pi = \\frac{\\sum_{i=1}^{N} \\gamma_{i}}{N} $ \n",
    "\n",
    "$\\frac{\\partial l}{\\partial \\sigma_{1}} = 0$  --> $\\sigma_{1}$ \n",
    "\n",
    "$\\frac{\\partial l}{\\partial \\sigma_{2}} = 0$  --> $\\sigma_{2}$ \n",
    "\n",
    "\n",
    "Start with some initial guess for $\\theta_{1,2}$, then find $\\gamma_{i}$ --> $\\theta_{1,2}$ and then iterate until fractional change is small.  \n",
    "\n",
    "## Expectation-Maximization Algorithm for M-component Gaussian Mixtures\n",
    "\n",
    "\n",
    "1) Take initial guess for $\\hat \\mu_{k}$, $\\hat \\sigma_{k}^{2}$, $\\hat f_{k}$, where $\\sum_{k=1}^{M} \\hat f_{k} = 1$, and $g(y) = \\sum_{k=1}^{M} f_{k} \\phi_{\\hat \\theta_{k}}(y)$ \n",
    "\n",
    "2) Expectation step: calculate responsibilities $\\hat \\gamma_{i}^{(k)} = \\frac{\\hat f_{k} \\phi_{\\hat \\theta_{k}}(y_{i})}{\\sum_{k=1}^{M}f_{k} \\phi_{\\hat \\theta_{k}}(y_{i})}$ \n",
    "\n",
    "3) Maximization step: $\\hat \\mu_{k} = \\frac{\\sum_{k=1}^{M} \\hat \\gamma_{i}^{(k)} y_{i}}{\\sum_{k=1}^{M}  \\hat \\gamma_{i}^{(k)}}$ and $\\hat \\sigma_{k}^{2} = \\frac{\\sum_{k=1}^{M} \\gamma_{i}^{(k)} (y_{i} - \\hat \\mu_{k})^{2}}{\\hat \\gamma_{i}}$, and $\\hat f_{k} = \\frac{\\sum_{k=1}^{N} \\hat \\gamma_{i}^{(k)}}{N}$ \n",
    "\n",
    "4) Iterate step 2 and 3 until fractional change in $\\theta$ < $\\epsilon$\n",
    "\n",
    "$L(\\theta_{k}) = \\Pi_{i=1}^{N} (\\sum_{k=1}^{M} f_{k} \\phi_{\\theta_{k}}(y_{i}))$ \n",
    "\n",
    "$ l(\\theta_{k}) = logL(\\theta_{k}) = \\sum_{i=1}^{N} log(\\sum_{k=1}^{M} f_{k} \\phi_{\\theta_{k}}(y_{i})) $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
