{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering \n",
    "\n",
    "Method for cheecking output from clustering: **Silhouettes** (Rousseeuw 1987). Suppose you have three different clusters, A, B, and C. You can look at average distance to cluster center w/in cluster, but also distance to other cluster pts. \n",
    "* a(i) = average distance of i to all other pts in A \n",
    "* d(i, C) = average distance of i to all other pts in cluster C (C not eq. A)\n",
    "    * if you have K clusters, you have k-1 values of d(i,C)\n",
    "    * b(i) = min of d(i, C)\n",
    "* if $\\lvert a(i) - b(i) \\rvert < \\epsilon$ then s(i) = 0 \n",
    "* if $a(i) < b(i)$ then $s(i) = 1 - \\frac{a(i)}{b(i)}$\n",
    "* if $a(i) > b(i)$ then $s(i) = \\frac{b(i)}{a(i)} - 1$ \n",
    "\n",
    "Plot these s(i) values against cluster number k. You want s(i) close to 1 to have \"good\" clustering (i.e. pts close to other pts in their own cluster, but far away from pts in other clusters). Choose some K (total number of clusters) such that $\\bar s_{k}$ is large. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Hierarchical Clustering: Example Python Code \n",
    "\n",
    "Initialize cluster list in 1D case. Each pt belongs to its own cluster \n",
    "Initialize distance with very large number, then iterate with some stopping condition.\n",
    "Check if pts are in same cluster (i==j) and if so continue, otherwise  compute distance. At end of two for loops you should know two closest clusters, and these are the ones you want to merge. Then we create a new cluster list that is length of previous cluster list, minus one (since we are doing a merger on each iteration of the while loop). Next we loop over old cluster list and if the cluster number is not the min_i or min_j we just copy it over to the new list. Then at the end we merge the min dist clusters and copy these over to the new cluster list. Finally, we need to overwrite the original cluster list. You can add some print statements at the end to print number of clusters, and the cluster list itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "very_large = 100000\n",
    "data = np.random.randint(1,size=1000)\n",
    "clusterlist = [0]*len(data)\n",
    "for i in range(len(data)): \n",
    "    clusterlist[i] = [data[i]]\n",
    "while len(clusterlist) > 1:\n",
    "    min_distance = very_large\n",
    "    for i in range(len(clusterlist)):\n",
    "        for j in range(len(clusterlist)):\n",
    "            if (i == j):\n",
    "                continue\n",
    "            else:\n",
    "                distance_i_j = distance(clusterlist[i],clusterlist[j])\n",
    "                if distance_i_j < min_distance:\n",
    "                    min_distance = distance_i_j\n",
    "                    min_i = i \n",
    "                    min_j = j\n",
    "    clusterlist_new = [0]*(len(clusterlist)-1)\n",
    "    k_new = 0 \n",
    "    for k in range(len(clusterlist)):\n",
    "        if (k != min_i) & (k != min_j):\n",
    "            clusterlist_new[k_new] = clusterlist[k]\n",
    "            k_new = k_new + 1 \n",
    "    \n",
    "    merged_cluster = clusterlist[min_i] + clusterlist[min_j]\n",
    "    clusterlist_new[k_new] = merged_cluster \n",
    "    clusterlist = clusterlist_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a distance metric \n",
    "\n",
    "Cluster 1 and cluster 2 are lists of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance(cluster1, cluster2):\n",
    "    d = 0.0\n",
    "    for i in range(len(cluster1)):\n",
    "        for j in range(len(cluster2)):\n",
    "            d = d + abs(cluster1[i] - cluster2[j])\n",
    "    # finding average distance \n",
    "    return d * 1.0/(len(cluster1)*len(cluster2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensional Reduction (PCA) \n",
    "\n",
    "Unsupervised learning consists of clustering (k-means, k-medoids, hierarchical clustering, gaussian mixtures, etc) and dimensional reduction (things like PCA). \n",
    "\n",
    "For PCA you are trying to find the parameters that maximize the variance of your data (first principal component). You can do this by rotating axes to, for instance, reduce it from 2D problem to 1D problem. You ultimately want to ignore components that have very small variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
