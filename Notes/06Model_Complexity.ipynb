{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Unsupervised Learning (Clustering)\n",
    "\n",
    "## Linear Algebra Review \n",
    "First some linear algebra review in prep for PCA: \n",
    "\n",
    "* $A^{T}$: transpose of matrix == interchange rows and columns \n",
    "* $(AB)^{T} = B^{T}A^{T}$ \n",
    "* $A^{-1}$ is some matrix A st $A A^{-1} = I$ and $A^{-1}A = I$\n",
    "* $(AB)^{-1} = B^{-1}A^{-1}$\n",
    "* $(A^{T})^{-1} = (A^{-1})^{T}$ \n",
    "* $(\\frac{\\partial x}{\\partial \\vec a}) = \\frac{\\partial x}{\\partial a_{i}}$\n",
    "* $(\\frac{\\partial \\vec a}{\\partial \\vec b})_{ij} = \\frac{\\partial a_{i}}{\\partial b_{j}}$\n",
    "* $\\frac{\\partial (\\vec x^{T} \\vec a)}{\\partial \\vec x} = \\frac{\\partial (\\vec a^{T} \\vec x)}{\\partial \\vec x} = \\vec a$ \n",
    "* $\\vec A \\vec u_{i} = \\lambda_{i} \\vec u_{i}$, where $u$ is eigenvec and $\\lambda$ is eigenval\n",
    "* symmetric matrix: $A^{T} = A$ \n",
    "* orthogonal matrix: $A^{T} = A^{-1}$\n",
    "* symmetric matrices have real eigenvalues and we can find orthonormal eigenvecs \n",
    "* **Lagrange Multipliers:** find maximum of $f(x_{1},x_{2})$ subject to constraint $g(x_{1},x_{2}) = 0$\n",
    "    * define some other function $\\phi(x_{1},x_{2}, \\lambda) = f(x_{1},x_{2}) + \\lambda g(x_{1}, x_{2})$, and then to find maximum you just take partial wrt variables. \n",
    "        * $\\frac{\\partial \\phi}{\\partial x_{1}} = \\frac{\\partial f}{\\partial x_{1}} + \\lambda \\frac{\\partial g}{\\partial x_{1}} = 0$ \n",
    "        * $\\frac{\\partial \\phi}{\\partial x_{2}} = \\frac{\\partial f}{\\partial x_{2}} + \\lambda \\frac{\\partial g}{\\partial x_{2}} = 0$ \n",
    "        * $\\frac{\\partial \\phi}{\\partial \\lambda} = g(x_{1},x_{2}) = 0$\n",
    "    \n",
    "    \n",
    "## Unsupervised Learning \n",
    "\n",
    "Clustering algorithms are used to see if there is some pattern in data. Gaussian mixutres (EM), k-means, k-medoids, hierarchical clustering. \n",
    "\n",
    "**In unsupervised learning there is really no way to check your answer.**\n",
    "\n",
    "### k-means clustering (otherwise known as \"yet another different k\")\n",
    "\n",
    "Let's say there are two clusters of data points in your parameter space:\n",
    "* decide value of k (in this example k=2)\n",
    "* start with random values for the centers of the k-cluster \n",
    "* for each point calculate the euclidean distance to the k-cluster centers. \n",
    "* assign the point membership to the closest cluster. \n",
    "* new cluster center for the n-th cluster is $ x^{(n)}_{center} = \\frac{\\sum x^{(n)}_{i}}{N_{n}}$ and $ y^{(n)}_{center} = \\frac{\\sum y^{(n)}_{i}}{N_{n}}$, where sum is over all i which belong to cluster n, and N is number of points in nth cluster. Much like calculating center of mass. \n",
    "* repeat distance and cluster center calculation until the assignments of data points do not change.\n",
    "* repeat many times for different random starting points, and delete 10% of points and run many times. \n",
    "\n",
    "### k-medoids clustering \n",
    "\n",
    "decide value of k (in this example k=2)\n",
    "* start with random values for the centers of the k-cluster \n",
    "* for each point calculate the distance to the k-cluster centers. \n",
    "* assign the point membership to the closest cluster. \n",
    "* new cluster center for the n-th cluster (medoid) is the data point which minizmizes the distance to all other pints in the n-th cluster. do this for n=1,2,3...k. Difference here from above is that this is an actual data point, whereas center before was more like a COM (not necessarily corresponding to a real data point). This is computationally more expensive, but also more robust. \n",
    "* repeat distance and cluster center calculation until the assignments of data points do not change.\n",
    "* repeat many times for different random starting points, and delete 10% of points and run many times.\n",
    "\n",
    "**This is less influenced by outliers, but it is computationally more expensive.** However, unless you have a HUGE number of points this will not actually be that much slower than doing a k-means clustering.  \n",
    "\n",
    "### Hierarchical clustering \n",
    "\n",
    "Drawbacks of k-means, gaussain mixture, k-medoids:\n",
    "\n",
    "* Must make choice of k \n",
    "* random starting configuration \n",
    "\n",
    "Hierarchical clustering does not suffer from these same drawbacks. In this method each point is initially its own cluster. You find its nearest neighbor and then make those two points a cluster. You then find the distance from this cluster to its nearest cluster and merge them. You create a binary tree, basically. Clusters at each level of the hierarchy are created by merging clusters at the lower level. At the lowest level, each cluster contains a single data point. At the highest level there is only one cluster containing all the data points. \n",
    "\n",
    "In general how do we decide distance between clusters? There are particular methods for doing this. For example, you can look at closest distance between any two points, average distance, or you can use the distance between furthest points. Shapes of clusters will change depending on how you calculate the distance. \n",
    "\n",
    "* single linkage (nearest neighbor, friend of friends)\n",
    "    * $d_{SL}(G, H) =$ min $d_{ii^{'}}$, where i is in G and $i^{'}$ is in H.\n",
    "* complete linkage (furthest neighbors)\n",
    "    * $d_{CL}(G,H) =$ max $d_{ii^{'}}$, where i is in G and $i^{'}$ is in H. \n",
    "* group average \n",
    "    * $d_{GA}(G,H) = \\frac{1}{N_{G}N_{H}} \\sum_{i~\\epsilon~G}\\sum_{i^{'}~\\epsilon~H} d_{ii^{'}}$ \n",
    "    \n",
    "Group average tends to be preferred method. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
