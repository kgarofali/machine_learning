{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Linear Models for Classification \n",
    "\n",
    "## Maximizing Likelihood\n",
    "\n",
    "$y = \\beta_{o} + \\beta_{1}x + \\epsilon$ \n",
    "\n",
    "**Likelihood**: $L(\\beta_{o}, \\beta_{1}) = \\Pi_{i=1} = \\frac{1}{\\sqrt{2\\pi}\\sigma_{i}}exp(-\\frac{y_{i}-(\\beta_{o}+\\beta_{1}x_{i})^{2}}{2\\sigma_{i}^{2}})$ \n",
    "\n",
    "Using something like gradient ascent or Monte Carlo methods you could get stuck in local maxima, but in general we can use such methods to get $\\nabla L$. Data is fixed, so vary above parameter values to try to find something reasonable.\n",
    "\n",
    "$logL = l(\\beta_{o}, \\beta_{1}) \\propto \\sum_{i=1}^{N} \\frac{-(y_{i} - (\\beta_{o} + \\beta_{1}x_{i}))^{2}}{2 \\sigma_{i}^{2}}$ \n",
    "\n",
    "Maximizing likelihood is product of exponentials, so it is sum of log of exponentials, which is sum of exponential exponents. If we remove minus sign and minimize above is same as least squares method, so here instead we are maximizing likelihood.\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is used for classification problems, because we cannot use linear regression for all problems, in particular those that have discrete values of y. So for classification problems we have some y = f(x) in which y is a discrete value, i.e. $y~\\epsilon[0,1]$. In these kinds of problems we set a threshold, so if $y > 0.5$: quasar or if $y < 0.5$: star. Example of this: **Fermi-Dirac function.** In stats lit is called \"logistic\" function. \n",
    "\n",
    "### Logistic Function \n",
    "\n",
    "$ f(x) = \\frac{e^{\\beta_{o} + \\beta_{1}x}}{1 + e^{\\beta_{o}+\\beta{1}x}} $ \n",
    "\n",
    "* for $x \\rightarrow \\inf, f \\rightarrow 1$ \n",
    "* for $x \\rightarrow -\\inf, f \\rightarrow 0$ (assuming $\\beta_{1} \\rightarrow 0$) \n",
    "\n",
    "Assume we have some training set of given y values (classes) given $x_{i}$, then using f(x) as our fitting function we will want to calculate the likelihood and maximize it to find our best values of $\\beta_{o}$ and $\\beta_{1}$. In the homework we will do this brute force by calculating a whole grid of likelihoods. \n",
    "\n",
    "### Likelihood \n",
    "\n",
    "$L(\\beta_{o}, \\beta_{1}) = \\Pi_{i: y_{i} = 1} p(x_{i}) \\Pi_{i^{'}:y_{i} = 0} (1- p(x_{i^{'}}))$ \n",
    "\n",
    "Given a list of $x_{i}$ with corresponding $y_{i}$, ys are only 0 or 1. So take all ys that are 1 and do product of corresponding probability, and do the same for category 0. Here probability p(x) corresponds to f(x) above. You will have two separate products (for each category), and multiplying these gives the likelihood. If we now take the log(L): \n",
    "\n",
    "$l(\\beta_{o},\\beta_{1}) = log(L(\\beta_{o},\\beta_{1}) = \\sum_{i=1}^{N} [y_{i} log(p(x_{i},\\beta_{o}\\beta_{1})) + (1- y_{i})log(1-p(x_{i},\\beta_{o}\\beta_{1}))]$\n",
    "\n",
    "Because $y_{i}$ is always 0 or 1 if $y_{i}$ is 1 the second term disappears, and vice-versa if $y_{i}$ is 0 we only get the second term. We are putting this in this form so that we can use gradient ascent method. So substituting in the p(x) from before: \n",
    "\n",
    "$1 - p(x_{i},\\beta_{o}\\beta_{1}) = \\frac{1}{1 + e^{\\beta_{o} + \\beta_{1}x_{i}}}$ \n",
    "\n",
    "$l(\\beta_{o}\\beta_{1}) = \\sum_{i=1}^{N} [ y_{i}(\\beta_{o} + \\beta_{1}x_{i}) - log(1+e^{\\beta_{o}+ \\beta_{1}x_{i}})]$\n",
    "\n",
    "If we use **gradient ascent** to maximize this, we need to take derivatives of these expressions. Gradient ascent or descent are greedy algorithms. So for ascent we take gradient and always move in the direction that is increasing. \n",
    "\n",
    "$\\nabla l = (\\frac{\\partial l}{\\partial \\beta_{o}}, \\frac{\\partial l}{\\partial \\beta_{1}})$ \n",
    "\n",
    "$\\frac{\\partial l}{\\partial \\beta_{o}} = \\sum_{i=1}^{N} (y_{i} - p(x_{i},\\beta_{o}\\beta_{1}))$ \n",
    "\n",
    "$\\frac{\\partial l}{\\partial \\beta_{1}} = \\sum_{i=1}^{N} x_{i}(y_{i} - p(x_{i},\\beta_{o}\\beta_{1}))$\n",
    "\n",
    "At each stage calculate gradient and take move in direction of maximum increase, where $\\nabla l$ is like a vector $\\frac{\\partial l}{\\partial \\beta_{o}} \\hat{i} + \\frac{\\partial l}{\\partial \\beta_{1}} \\hat{j}$. So, for example, start with $\\beta_{o} = 1, \\beta_{1} = 1$ as first iteration ($\\vec \\beta^{1}$) and do $\\nabla l$, and then next iteration will be $\\vec \\beta^{2} = \\vec \\beta^{1} + \\alpha \\nabla l$. We could get stuck in local maxima here unless we try different starting points--Monte Carlo can avoid this potentially by not always going to increasing direction.\n",
    "\n",
    "## Bootstrap Resampling Method \n",
    "\n",
    "Suppose you have lots of data, and you want to find the mean. In general we can do this with $\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_{i}$. But how much can we trust this value? This is a central issue in statistics. The bootstrap method seeks to take our sample and generate a new sample. For example, N times choose an $x_{i}$ randomly with replacement to make a new sample. It will be a similar sample as the original, except that some values will be repeated and some will be missing. Let's call the new samples $B_{1}$, $B_{2}$. Assuming $x_{i}$ are all different and you have N values, how many new samples could you generate? $N^{N}$, which will end up being a *very* large number. Usually just limited by computing power. Let's say you generate 1000 resampled arrays, then you can calculate the mean for each of these and make a histogram. This histogram will give you an estimate of how much you can trust your mean, $\\mu$. If it is very broad for example, you may not be able to trust your value very much. However, if it is narrow your mean may be more reliable. \n",
    "\n",
    "You could do bootstrap resampling for pairs of values, like $\\beta_{o}$ and $\\beta_{1}$. To use this method you need to have a sufficiently large and representative sample to begin with in order to make a meaningful conclusion--nothing can cure lack of data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
