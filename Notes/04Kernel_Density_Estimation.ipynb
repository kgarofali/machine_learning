{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Kernel density estimation and classification (Naive Bayes Classifier) \n",
    "\n",
    "Put in a function (e.g. gaussian) at each data point and add them to get some smooth distribution. We want a better way to get a probability distribution function as opposed to just binning and doing a histogram. \n",
    "\n",
    "$ f(x) = \\frac{1}{N}\\sum_{i=1}^{N} K_{H}( \\vert \\vert \\vec x - \\vec x_{i} \\vert \\vert)$, where N is  total num of points that you have, h is bandwith, and the term in parenthesis is the \"distance.\" This sum will give you an estimate of the PDF. The distance is given by: \n",
    "\n",
    "$\\vert \\vert \\vec x - \\vec x_{i} \\vert \\vert = \\sqrt{[(x_{1} - x_{i1})^{2} + (x_{2} - x_{i2})^{2} + (x_{3} - x_{i3})^{2}  +(x_{4} - x_{i4})^{2}]}$\n",
    "\n",
    "$ K_{h}(\\vert \\vert \\vec x - \\vec x_{i} \\vert \\vert) = [\\frac{1}{\\sqrt{2\\pi} h}]^{d} e^{-\\frac{(\\vert \\vert \\vec x - \\vec x_{i} \\vert \\vert)^{2}}{2h^{2}}}$, where d is the dimension. \n",
    "\n",
    "How do we choose the bandwidth in general? If it tends to zero then we get a delta function. We don't want this. If it is too broad then density at far away points is affecting more nearby points--also do not want this. A good way to select this then is to use cross-validation (i.e. for doing a classification problem). \n",
    "\n",
    "## Epanechnikov Kernel \n",
    "\n",
    "A different kernel choice, other than gaussian: \n",
    "$K_{h}(z) = \\frac{3}{4h}(1 - \\frac{z^{2}}{h^{2}})$, if $\\frac{z^{2}}{h^{2}} \\leq 1$, otherwise this is zero. \n",
    "\n",
    "## Classification with KDE \n",
    "\n",
    "Assume you have a training set of quasars $\\vec x$ and a training set of stars $\\vec x$. The $P_{quasar}(\\vec x)$ and $P_{star}(\\vec)$ is what we are trying to find at some new points. So with KDE we can get $P(\\vec x \\vert quasar)$ or $P(\\vec x \\vert star)$. \n",
    "\n",
    "We can use Bayesian approach: $P(star \\vert \\vec x) = \\frac{P(star)P(\\vec x \\vert star)}{P(star) P(\\vec x \\vert star) + P(quasar)P(\\vec x \\vert quasar)}$. \n",
    "\n",
    "This tells us that given some $\\vec x$ we want to know what the prob that it is a star. P(star) is our prior (from previous studies, earlier work, etc), and $P(\\vec x \\vert star)$ is our likelihood that we get from KDE. Denominator is normalization constant.\n",
    "\n",
    "### Naive Bayesian Approach \n",
    "\n",
    "Factorize as follows: \n",
    "$ P(\\vec x \\vert star) = P(x_{1} \\vert star) P(x_{2} \\vert star) P(x_{3} \\vert star) P(x_{4} \\vert star)$\n",
    "\n",
    "$P(c1 \\vert \\vec x) = N P(c1)P(\\vec x \\vert c1)$, where P(c1) is prior and $P(\\vec x \\vert c1)$ is likelihood. \n",
    "\n",
    "$P(c2 \\vert \\vec x) = N P(c2)P(\\vec x \\vert c2)$\n",
    "\n",
    "$P(c1 \\vert \\vec x) + P(c2 \\vert \\vec x) = 1$ \n",
    "\n",
    "$N[P(c1) P(\\vec x \\vert c1) + P(c2) P(\\vec x \\vert c2)] = 1$ \n",
    "\n",
    "These methods are powerful because we're not making any assumptions about our quasars or stars, for example. The choice of kernel doesn't matter. \n",
    "\n",
    "# K-nearest neighbor classification \n",
    "\n",
    "Assume you have some training set $\\vec x$ where each val has a classification $\\vec y$ (say zero or one). You can define some metric: $\\vert \\vert \\vec x - \\vec x_{i} \\vert \\vert$. A common metric is the Euclidean metric: $ [(x_{1} - x_{i1})^{2} + (x_{2} - x_{i2})^{2} + ...]^{1/2} $. \n",
    "\n",
    "In our function approx we are doing: $f(\\vec x)$ = same class as class of nearest neighbor of $\\vec x$ in the training set. This example is for k = 1 (nearest neighbor), but you can look at say k = 5. Rule: k = odd and use majority rule. You can choose k using x-fold cross validation--this can give you optimum k. This is a classic bias-variance trade-off, so in general try to use k somwhere in the middle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
